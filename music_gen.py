# -*- coding: utf-8 -*-
"""Music_Gen.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/186OILh9USSrJlQrG5CBEkTxE86DiIci7
"""

!pip install pretty_midi

import pretty_midi
import matplotlib.pyplot as plt

# Load the generated MIDI file
midi_file = f"flute_ai_output_{user_key}.mid"
pm = pretty_midi.PrettyMIDI(midi_file)

# Create lists for note start times, end times, and pitches
note_starts = []
note_ends = []
note_pitches = []

for inst in pm.instruments:
    for note in inst.notes:
        note_starts.append(note.start)
        note_ends.append(note.end)
        note_pitches.append(note.pitch)

# Plot notes as horizontal lines (piano-roll style)
plt.figure(figsize=(12, 6))
for start, end, pitch in zip(note_starts, note_ends, note_pitches):
    plt.hlines(pitch, start, end, colors='blue', linewidth=2)

plt.xlabel("Time (seconds)")
plt.ylabel("Pitch (MIDI note number)")
plt.title("Generated Flute Melody")
plt.grid(True)
plt.show()

from google.colab import files

uploaded = files.upload()

!unzip -q MIDI_files.zip -d MIDI_files

"""
Hybrid Markov (rhythm) + LSTM (pitch) flute melody generator.
Generates realistic flute melodies with rhythmic variety and smooth melodic motion.
"""
import glob, os, random
from collections import defaultdict
import pretty_midi
import torch
import torch.nn as nn
import torch.optim as optim


# -------------------------
#  CONFIG
# -------------------------
midi_folder = "/content/MIDI_files/MIDI_files"  # <-- change this if needed
device = "cuda" if torch.cuda.is_available() else "cpu"

MARKOV_ORDER = 3
MARKOV_GEN_LEN = 250
SEQ_LEN = 64
BATCH_SIZE = 32
EPOCHS = 25
LR = 3e-4
TEMPERATURE = 0.9

# -------------------------
#  UTILITIES
# -------------------------
def quantize(x, step=0.25):
    return round(round(x / step) * step, 2)

def midi_to_tokens(file):
    pm = pretty_midi.PrettyMIDI(file)
    notes = []
    for inst in pm.instruments:
        if inst.is_drum:
            continue
        for n in inst.notes:
            notes.append((n.start, n.end, n.pitch))
    notes.sort()
    tokens, last_time = [], 0.0
    for start, end, pitch in notes:
        delta = quantize(start - last_time)
        dur = quantize(end - start)
        if dur <= 0:
            continue
        if delta >= 1.0:
            tokens.append("REST_1.0")
        if delta > 0:
            tokens.append(f"TS_{delta}")
        tokens.append(f"NOTE_{pitch}_DUR_{dur}")
        last_time = end
    return tokens

def tokens_to_midi(tokens, out_file="flute_ai_output.mid"):
    pm = pretty_midi.PrettyMIDI()
    inst = pretty_midi.Instrument(program=73)  # Flute
    time = 0.0
    for t in tokens:
        if t.startswith("TS_"):
            time += float(t.split("_")[1])
        elif t.startswith("REST_"):
            time += float(t.split("_")[1])
        elif t.startswith("NOTE_"):
            _, pitch, _, dur = t.split("_")
            pitch = int(pitch)
            dur = float(dur)
            inst.notes.append(pretty_midi.Note(
                velocity=90, pitch=pitch, start=time, end=time+dur))
            time += dur
    pm.instruments.append(inst)
    pm.write(out_file)
    return out_file

# -------------------------
#  LOAD DATASET
# -------------------------
files = glob.glob(os.path.join(midi_folder, "*.mid"))
if not files:
    raise RuntimeError(f"No MIDI files found in {midi_folder}")
print(f"Found {len(files)} MIDI files.")

all_tokens = []
for f in files:
    try:
        all_tokens += midi_to_tokens(f)
    except Exception as e:
        print(f"Skipping {f}: {e}")

# Create rhythm-only tokens
rhythm_tokens = []
for t in all_tokens:
    if t.startswith("NOTE_"):
        parts = t.split("_")
        dur = parts[-1]  # safer than parts[3]
        rhythm_tokens.append(f"NOTE_DUR_{dur}")
    else:
        rhythm_tokens.append(t)

# -------------------------
#  MARKOV MODEL (RHYTHM)
# -------------------------
def build_markov(tokens, order=3):
    model = defaultdict(list)
    for i in range(len(tokens) - order):
        key = tuple(tokens[i:i+order])
        model[key].append(tokens[i + order])
    return model  # keep defaultdict

def generate_markov(model, start_token=None, length=200):
    order = len(next(iter(model)))

    # --- Initialize starting state ---
    if start_token is None:
        # choose a real state from the model
        current = random.choice(list(model.keys()))
    else:
        # replicate start token n times to match Markov order
        current = tuple([start_token] * order)

    result = list(current)

    # --- Generate sequence ---
    for _ in range(length):
        choices = model.get(current)

        # If no continuation exists, pick a new valid state
        if not choices:
            current = random.choice(list(model.keys()))
            choices = model[current]

        nxt = random.choice(choices)
        result.append(nxt)

        # slide window forward efficiently
        current = (*current[1:], nxt)

    return result

# -------------------------
#  LSTM MODEL (PITCH)
# -------------------------
pitches = sorted({int(t.split("_")[1]) for t in all_tokens if t.startswith("NOTE_")})
stoi = {p: i for i, p in enumerate(pitches)}
itos = {i: p for p, i in stoi.items()}

class PitchLSTM(nn.Module):
    def __init__(self, vocab, embed=64, hidden=128):
        super().__init__()
        self.embed = nn.Embedding(vocab, embed)
        self.lstm = nn.LSTM(embed, hidden, num_layers=2, batch_first=True)
        self.fc = nn.Linear(hidden, vocab)

    def forward(self, x, h=None):
        x = self.embed(x)
        out, h = self.lstm(x, h)
        out = self.fc(out)
        return out, h


model = PitchLSTM(len(pitches)).to(device)
opt = optim.Adam(model.parameters(), lr=LR)
loss_fn = nn.CrossEntropyLoss()

# Encode all pitches
encoded = [stoi[int(t.split("_")[1])] for t in all_tokens if t.startswith("NOTE_")]
encoded = torch.tensor(encoded, dtype=torch.long)

# -------------------------
#  Correct overlapping chunks
# -------------------------
chunks = []
for i in range(len(encoded) - SEQ_LEN):
    seq = encoded[i:i + SEQ_LEN]
    x = seq[:-1]       # length SEQ_LEN-1
    y = seq[1:]        # next-token targets
    chunks.append((x, y))

print(f"Created {len(chunks)} chunks of length {SEQ_LEN-1}")

# -------------------------
#  Batch loader
# -------------------------
def get_batch(bs=BATCH_SIZE):
    idx = torch.randint(0, len(chunks), (bs,))
    x = torch.stack([chunks[i][0] for i in idx])
    y = torch.stack([chunks[i][1] for i in idx])
    return x.to(device), y.to(device)


# -------------------------
#  Training Loop
# -------------------------
for e in range(1, EPOCHS + 1):
    x, y = get_batch()
    opt.zero_grad()

    out, _ = model(x)  # out: (B, T, vocab)
    loss = loss_fn(out.reshape(-1, len(pitches)), y.reshape(-1))

    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # optional but recommended
    opt.step()

    if e % 5 == 0:
        print(f"Epoch {e}/{EPOCHS}  loss={loss.item():.4f}")


# -------------------------
#  RULE FILTER (KEY + REPETITION + SMOOTHING)
# -------------------------
def apply_rule_filters(tokens, key="C"):
    major_keys = {
        "C":  [60, 62, 64, 65, 67, 69, 71, 72],
        "Bb": [58, 60, 62, 63, 65, 67, 69, 70],
        "G":  [55, 57, 59, 60, 62, 64, 66, 67]
    }
    scale_notes = major_keys.get(key, major_keys["C"]) #defaults to C if invalid key choice
    filtered, last_pitch = [], None
    repeat_count, max_repeats = 0, 3

    for t in tokens:
        if t.startswith("NOTE_"):
            parts = t.split("_")
            pitch = int(parts[1])
            dur = float(parts[3])

            # Stay in key
            if pitch not in scale_notes:
                pitch = min(scale_notes, key=lambda x: abs(x - pitch))
            # Avoid long repeats
            if pitch == last_pitch:
                repeat_count += 1
                if repeat_count > max_repeats:
                    continue
            else:
                repeat_count = 0
            # Avoid large leaps
            if last_pitch is not None and abs(pitch - last_pitch) > 24:
                pitch = last_pitch + (12 if pitch > last_pitch else -24)
            # Limit duration
            dur = max(0.1, min(dur, 1.0))
            filtered.append(f"NOTE_{pitch}_DUR_{dur}")
            last_pitch = pitch
        elif t.startswith("TS_") or t.startswith("REST_"):
            filtered.append(t)
    return filtered

# -------------------------
#  GENERATE MELODY
# -------------------------
available_keys = ["C", "Bb", "G"]
print("Available keys:", ", ".join(available_keys))
user_key = input("Enter desired key (C/Bb/G): ").strip().title()
if user_key not in available_keys:
    user_key = "C"

print(f" Generating melody in {user_key} major...")

start_tok = random.choice(rhythm_tokens)
rhythm_seq = generate_markov(markov, start_tok, length=MARKOV_GEN_LEN)


# -----------------------------------------
#   Helper: Penalize large pitch jumps
# -----------------------------------------
def bias_large_leaps(logits, prev_idx, threshold=5, penalty=0.8):
    idx = torch.arange(len(logits), device=logits.device)
    dist = (idx - prev_idx).abs()
    mask = (dist > threshold).float()  # only penalize leaps > threshold
    return logits - penalty * dist * mask

# -------------------------
#  PITCH GENERATION
# -------------------------
model.eval()
gen_pitches = []

# start randomly
x = torch.randint(0, len(pitches), (1, 1)).to(device)
prev_pitch_idx = x.item()   # <-- needed for penalty
h = None

for r in rhythm_seq:
    if "NOTE_DUR_" in r:
        with torch.no_grad():

            # Standard forward pass
            out, h = model(x, h)
            logits = out[0, -1]  # shape (vocab,)

            logits = bias_large_leaps(logits, prev_pitch_idx, penalty=1.5)

            # Softmax sampling
            probs = torch.softmax(logits / TEMPERATURE, dim=0)
            next_id = torch.multinomial(probs, 1).item()

        gen_pitches.append(itos[next_id])

        # prep next input
        x = torch.tensor([[next_id]], device=device)
        prev_pitch_idx = next_id  # <-- update index

    else:
        gen_pitches.append(None)


# -------------------------
#  MERGE RHYTHM + PITCH
# -------------------------
tokens = []
p_iter = iter([p for p in gen_pitches if p is not None])

for r in rhythm_seq:
    if r.startswith("NOTE_DUR_"):
        dur = r.split("_")[2]
        pitch = next(p_iter)
        tokens.append(f"NOTE_{pitch}_DUR_{dur}")
    else:
        tokens.append(r)

# Apply cleanup filters
tokens = apply_rule_filters(tokens, key=user_key)


# -------------------------
# Limit consecutive rests
# -------------------------
def limit_consecutive_rests(tokens, max_rests=3):
    filtered = []
    rest_count = 0
    for t in tokens:
        if t.startswith("REST_"):
            rest_count += 1
            if rest_count > max_rests:
                continue  # skip extra rests
        else:
            rest_count = 0
        filtered.append(t)
    return filtered

tokens = limit_consecutive_rests(tokens, max_rests=3)
# Save output MIDI
out_path = f"flute_ai_output_{user_key}.mid"
tokens_to_midi(tokens, out_path)
print(f" Saved melody to {out_path}")

!apt-get update -qq
!apt-get install -y timidity

from IPython.display import Audio, display
from google.colab import files
import ipywidgets as widgets

midi_file = f"flute_ai_output_{user_key}.mid"
wav_file = f"flute_ai_output_{user_key}.wav"

# Convert MIDI → WAV
!timidity "$midi_file" -Ow -o "$wav_file"

# Play audio
display(Audio(wav_file))

# Create Yes/No buttons
button_yes = widgets.Button(description="Yes")
button_no = widgets.Button(description="No")
output = widgets.Output()

def on_yes_clicked(b):
    with output:
        print("Downloading...")
        files.download(wav_file)

def on_no_clicked(b):
    with output:
        print("Download canceled.")

button_yes.on_click(on_yes_clicked)
button_no.on_click(on_no_clicked)

display(widgets.HBox([button_yes, button_no]), output)

import pretty_midi
import matplotlib.pyplot as plt
import numpy as np

# Load MIDI
midi_file = f"flute_ai_output_{user_key}.mid"
pm = pretty_midi.PrettyMIDI(midi_file)

# Collect notes
note_starts = []
note_ends = []
note_pitches = []

for inst in pm.instruments:
    for note in inst.notes:
        note_starts.append(note.start)
        note_ends.append(note.end)
        note_pitches.append(note.pitch)

note_starts = np.array(note_starts)
note_ends = np.array(note_ends)
note_pitches = np.array(note_pitches)

# Determine plot limits
min_pitch = note_pitches.min() - 2
max_pitch = note_pitches.max() + 2

min_time = note_starts.min()
max_time = note_ends.max()

# Plot
plt.figure(figsize=(14, 6), dpi=130)

# Use pitch-based colormap for visibility
colors = plt.cm.viridis((note_pitches - min_pitch) / (max_pitch - min_pitch))

for start, end, pitch, c in zip(note_starts, note_ends, note_pitches, colors):
    plt.hlines(pitch, start, end,
               color=c,
               linewidth=2,
               alpha=0.9)

plt.xlabel("Time (seconds)", fontsize=12)
plt.ylabel("Pitch (MIDI note)", fontsize=12)
plt.title("Generated Flute Melody – Pitch Across Time", fontsize=14)

plt.xlim(min_time, max_time)
plt.ylim(min_pitch, max_pitch)

plt.grid(True, linestyle="--", alpha=0.4)
plt.tight_layout()
plt.show()